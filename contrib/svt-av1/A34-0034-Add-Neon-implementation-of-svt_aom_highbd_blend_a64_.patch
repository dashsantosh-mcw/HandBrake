From 482a91997420c7f27bcf3f14d061c815b39ae4db Mon Sep 17 00:00:00 2001
From: Salome Thirot <salome.thirot@arm.com>
Date: Thu, 24 Oct 2024 16:41:31 +0100
Subject: [PATCH 34/56] Add Neon implementation of
 svt_aom_highbd_blend_a64_d16_mask_neon

Port the libaom Neon implementation of
svt_aom_highbd_blend_a64_d16_mask_neon and add the corresponding unit
tests.
---
 .../Lib/ASM_NEON/highbd_blend_a64_mask_neon.c | 235 ++++++++++++++++++
 Source/Lib/Codec/common_dsp_rtcd.c            |   2 +-
 Source/Lib/Codec/common_dsp_rtcd.h            |   1 +
 test/CompoundUtilTest.cc                      |   7 +
 4 files changed, 244 insertions(+), 1 deletion(-)

diff --git a/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c b/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
index c68feff8..f13491cb 100644
--- a/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
@@ -223,3 +223,238 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
         }
     }
 }
+
+#define HBD_BLEND_A64_D16_MASK(bd, round0_bits)                                                                       \
+    static inline uint16x8_t alpha_##bd##_blend_a64_d16_u16x8(                                                        \
+        uint16x8_t m, uint16x8_t a, uint16x8_t b, int32x4_t round_offset) {                                           \
+        const uint16x8_t m_inv = vsubq_u16(vdupq_n_u16(AOM_BLEND_A64_MAX_ALPHA), m);                                  \
+                                                                                                                      \
+        uint32x4_t blend_u32_lo = vmlal_u16(vreinterpretq_u32_s32(round_offset), vget_low_u16(m), vget_low_u16(a));   \
+        uint32x4_t blend_u32_hi = vmlal_u16(vreinterpretq_u32_s32(round_offset), vget_high_u16(m), vget_high_u16(a)); \
+                                                                                                                      \
+        blend_u32_lo = vmlal_u16(blend_u32_lo, vget_low_u16(m_inv), vget_low_u16(b));                                 \
+        blend_u32_hi = vmlal_u16(blend_u32_hi, vget_high_u16(m_inv), vget_high_u16(b));                               \
+                                                                                                                      \
+        uint16x4_t blend_u16_lo = vqrshrun_n_s32(                                                                     \
+            vreinterpretq_s32_u32(blend_u32_lo),                                                                      \
+            AOM_BLEND_A64_ROUND_BITS + 2 * FILTER_BITS - round0_bits - COMPOUND_ROUND1_BITS);                         \
+        uint16x4_t blend_u16_hi = vqrshrun_n_s32(                                                                     \
+            vreinterpretq_s32_u32(blend_u32_hi),                                                                      \
+            AOM_BLEND_A64_ROUND_BITS + 2 * FILTER_BITS - round0_bits - COMPOUND_ROUND1_BITS);                         \
+                                                                                                                      \
+        uint16x8_t blend_u16 = vcombine_u16(blend_u16_lo, blend_u16_hi);                                              \
+        blend_u16            = vminq_u16(blend_u16, vdupq_n_u16((1 << bd) - 1));                                      \
+                                                                                                                      \
+        return blend_u16;                                                                                             \
+    }                                                                                                                 \
+                                                                                                                      \
+    static inline void highbd_##bd##_blend_a64_d16_mask_neon(uint16_t            *dst,                                \
+                                                             uint32_t             dst_stride,                         \
+                                                             const CONV_BUF_TYPE *src0,                               \
+                                                             uint32_t             src0_stride,                        \
+                                                             const CONV_BUF_TYPE *src1,                               \
+                                                             uint32_t             src1_stride,                        \
+                                                             const uint8_t       *mask,                               \
+                                                             uint32_t             mask_stride,                        \
+                                                             int                  w,                                  \
+                                                             int                  h,                                  \
+                                                             int                  subw,                               \
+                                                             int                  subh) {                                              \
+        const int offset_bits  = bd + 2 * FILTER_BITS - round0_bits;                                                  \
+        int32_t   round_offset = (1 << (offset_bits - COMPOUND_ROUND1_BITS)) +                                        \
+            (1 << (offset_bits - COMPOUND_ROUND1_BITS - 1));                                                          \
+        int32x4_t offset = vdupq_n_s32(-(round_offset << AOM_BLEND_A64_ROUND_BITS));                                  \
+                                                                                                                      \
+        if ((subw | subh) == 0) {                                                                                     \
+            if (w >= 8) {                                                                                             \
+                do {                                                                                                  \
+                    int i = 0;                                                                                        \
+                    do {                                                                                              \
+                        uint16x8_t m0 = vmovl_u8(vld1_u8(mask + i));                                                  \
+                        uint16x8_t s0 = vld1q_u16(src0 + i);                                                          \
+                        uint16x8_t s1 = vld1q_u16(src1 + i);                                                          \
+                                                                                                                      \
+                        uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m0, s0, s1, offset);                      \
+                                                                                                                      \
+                        vst1q_u16(dst + i, blend);                                                                    \
+                        i += 8;                                                                                       \
+                    } while (i < w);                                                                                  \
+                                                                                                                      \
+                    mask += mask_stride;                                                                              \
+                    src0 += src0_stride;                                                                              \
+                    src1 += src1_stride;                                                                              \
+                    dst += dst_stride;                                                                                \
+                } while (--h != 0);                                                                                   \
+            } else {                                                                                                  \
+                do {                                                                                                  \
+                    uint16x8_t m0 = vmovl_u8(load_unaligned_u8_4x2(mask, mask_stride));                               \
+                    uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);                                        \
+                    uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);                                        \
+                                                                                                                      \
+                    uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m0, s0, s1, offset);                          \
+                                                                                                                      \
+                    store_u16x4_strided_x2(dst, dst_stride, blend);                                                   \
+                                                                                                                      \
+                    mask += 2 * mask_stride;                                                                          \
+                    src0 += 2 * src0_stride;                                                                          \
+                    src1 += 2 * src1_stride;                                                                          \
+                    dst += 2 * dst_stride;                                                                            \
+                    h -= 2;                                                                                           \
+                } while (h != 0);                                                                                     \
+            }                                                                                                         \
+        } else if ((subw & subh) == 1) {                                                                              \
+            if (w >= 8) {                                                                                             \
+                do {                                                                                                  \
+                    int i = 0;                                                                                        \
+                    do {                                                                                              \
+                        uint8x16_t m0 = vld1q_u8(mask + 0 * mask_stride + 2 * i);                                     \
+                        uint8x16_t m1 = vld1q_u8(mask + 1 * mask_stride + 2 * i);                                     \
+                        uint16x8_t s0 = vld1q_u16(src0 + i);                                                          \
+                        uint16x8_t s1 = vld1q_u16(src1 + i);                                                          \
+                                                                                                                      \
+                        uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(                                        \
+                            vget_low_u8(m0), vget_low_u8(m1), vget_high_u8(m0), vget_high_u8(m1)));                   \
+                        uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                   \
+                                                                                                                      \
+                        vst1q_u16(dst + i, blend);                                                                    \
+                        i += 8;                                                                                       \
+                    } while (i < w);                                                                                  \
+                                                                                                                      \
+                    mask += 2 * mask_stride;                                                                          \
+                    src0 += src0_stride;                                                                              \
+                    src1 += src1_stride;                                                                              \
+                    dst += dst_stride;                                                                                \
+                } while (--h != 0);                                                                                   \
+            } else {                                                                                                  \
+                do {                                                                                                  \
+                    uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride);                                                  \
+                    uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);                                                  \
+                    uint8x8_t  m2 = vld1_u8(mask + 2 * mask_stride);                                                  \
+                    uint8x8_t  m3 = vld1_u8(mask + 3 * mask_stride);                                                  \
+                    uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);                                        \
+                    uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);                                        \
+                                                                                                                      \
+                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(m0, m1, m2, m3));                           \
+                    uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                       \
+                                                                                                                      \
+                    store_u16x4_strided_x2(dst, dst_stride, blend);                                                   \
+                                                                                                                      \
+                    mask += 4 * mask_stride;                                                                          \
+                    src0 += 2 * src0_stride;                                                                          \
+                    src1 += 2 * src1_stride;                                                                          \
+                    dst += 2 * dst_stride;                                                                            \
+                    h -= 2;                                                                                           \
+                } while (h != 0);                                                                                     \
+            }                                                                                                         \
+        } else if (subw == 1 && subh == 0) {                                                                          \
+            if (w >= 8) {                                                                                             \
+                do {                                                                                                  \
+                    int i = 0;                                                                                        \
+                    do {                                                                                              \
+                        uint8x8_t  m0 = vld1_u8(mask + 2 * i);                                                        \
+                        uint8x8_t  m1 = vld1_u8(mask + 2 * i + 8);                                                    \
+                        uint16x8_t s0 = vld1q_u16(src0 + i);                                                          \
+                        uint16x8_t s1 = vld1q_u16(src1 + i);                                                          \
+                                                                                                                      \
+                        uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));                                 \
+                        uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                   \
+                                                                                                                      \
+                        vst1q_u16(dst + i, blend);                                                                    \
+                        i += 8;                                                                                       \
+                    } while (i < w);                                                                                  \
+                                                                                                                      \
+                    mask += mask_stride;                                                                              \
+                    src0 += src0_stride;                                                                              \
+                    src1 += src1_stride;                                                                              \
+                    dst += dst_stride;                                                                                \
+                } while (--h != 0);                                                                                   \
+            } else {                                                                                                  \
+                do {                                                                                                  \
+                    uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride);                                                  \
+                    uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);                                                  \
+                    uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);                                        \
+                    uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);                                        \
+                                                                                                                      \
+                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));                                     \
+                    uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                       \
+                                                                                                                      \
+                    store_u16x4_strided_x2(dst, dst_stride, blend);                                                   \
+                                                                                                                      \
+                    mask += 2 * mask_stride;                                                                          \
+                    src0 += 2 * src0_stride;                                                                          \
+                    src1 += 2 * src1_stride;                                                                          \
+                    dst += 2 * dst_stride;                                                                            \
+                    h -= 2;                                                                                           \
+                } while (h != 0);                                                                                     \
+            }                                                                                                         \
+        } else {                                                                                                      \
+            if (w >= 8) {                                                                                             \
+                do {                                                                                                  \
+                    int i = 0;                                                                                        \
+                    do {                                                                                              \
+                        uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride + i);                                          \
+                        uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride + i);                                          \
+                        uint16x8_t s0 = vld1q_u16(src0 + i);                                                          \
+                        uint16x8_t s1 = vld1q_u16(src1 + i);                                                          \
+                                                                                                                      \
+                        uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0, m1));                                          \
+                        uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                   \
+                                                                                                                      \
+                        vst1q_u16(dst + i, blend);                                                                    \
+                        i += 8;                                                                                       \
+                    } while (i < w);                                                                                  \
+                                                                                                                      \
+                    mask += 2 * mask_stride;                                                                          \
+                    src0 += src0_stride;                                                                              \
+                    src1 += src1_stride;                                                                              \
+                    dst += dst_stride;                                                                                \
+                } while (--h != 0);                                                                                   \
+            } else {                                                                                                  \
+                do {                                                                                                  \
+                    uint8x8_t  m0_2 = load_unaligned_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);                 \
+                    uint8x8_t  m1_3 = load_unaligned_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);                 \
+                    uint16x8_t s0   = load_unaligned_u16_4x2(src0, src0_stride);                                      \
+                    uint16x8_t s1   = load_unaligned_u16_4x2(src1, src1_stride);                                      \
+                                                                                                                      \
+                    uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0_2, m1_3));                                          \
+                    uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                       \
+                                                                                                                      \
+                    store_u16x4_strided_x2(dst, dst_stride, blend);                                                   \
+                                                                                                                      \
+                    mask += 4 * mask_stride;                                                                          \
+                    src0 += 2 * src0_stride;                                                                          \
+                    src1 += 2 * src1_stride;                                                                          \
+                    dst += 2 * dst_stride;                                                                            \
+                    h -= 2;                                                                                           \
+                } while (h != 0);                                                                                     \
+            }                                                                                                         \
+        }                                                                                                             \
+    }
+
+// 10 bitdepth
+HBD_BLEND_A64_D16_MASK(10, ROUND0_BITS)
+// 8 bitdepth
+HBD_BLEND_A64_D16_MASK(8, ROUND0_BITS)
+
+void svt_aom_highbd_blend_a64_d16_mask_neon(uint8_t *dst_8, uint32_t dst_stride, const CONV_BUF_TYPE *src0,
+                                            uint32_t src0_stride, const CONV_BUF_TYPE *src1, uint32_t src1_stride,
+                                            const uint8_t *mask, uint32_t mask_stride, int w, int h, int subw, int subh,
+                                            ConvolveParams *conv_params, const int bd) {
+    (void)conv_params;
+    assert(h >= 1);
+    assert(w >= 1);
+    assert(IS_POWER_OF_TWO(h));
+    assert(IS_POWER_OF_TWO(w));
+
+    uint16_t *dst = (uint16_t *)dst_8;
+    assert(IMPLIES(src0 == dst, src0_stride == dst_stride));
+    assert(IMPLIES(src1 == dst, src1_stride == dst_stride));
+
+    if (bd == 10) {
+        highbd_10_blend_a64_d16_mask_neon(
+            dst, dst_stride, src0, src0_stride, src1, src1_stride, mask, mask_stride, w, h, subw, subh);
+    } else {
+        highbd_8_blend_a64_d16_mask_neon(
+            dst, dst_stride, src0, src0_stride, src1, src1_stride, mask, mask_stride, w, h, subw, subh);
+    }
+}
diff --git a/Source/Lib/Codec/common_dsp_rtcd.c b/Source/Lib/Codec/common_dsp_rtcd.c
index 5739bf33..580c0c87 100644
--- a/Source/Lib/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Codec/common_dsp_rtcd.c
@@ -1005,7 +1005,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_ONLY_C(svt_aom_highbd_blend_a64_vmask_8bit, svt_aom_highbd_blend_a64_vmask_8bit_c);
     SET_ONLY_C(svt_aom_highbd_blend_a64_vmask_16bit, svt_aom_highbd_blend_a64_vmask_16bit_c);
     SET_ONLY_C(svt_aom_highbd_blend_a64_hmask_16bit, svt_aom_highbd_blend_a64_hmask_16bit_c);
-    SET_ONLY_C(svt_aom_highbd_blend_a64_d16_mask, svt_aom_highbd_blend_a64_d16_mask_c);
+    SET_NEON(svt_aom_highbd_blend_a64_d16_mask, svt_aom_highbd_blend_a64_d16_mask_c, svt_aom_highbd_blend_a64_d16_mask_neon);
     SET_NEON(svt_cfl_predict_lbd, svt_cfl_predict_lbd_c, svt_aom_cfl_predict_lbd_neon);
     SET_ONLY_C(svt_cfl_predict_hbd, svt_cfl_predict_hbd_c);
     SET_NEON(svt_av1_filter_intra_predictor, svt_av1_filter_intra_predictor_c, svt_av1_filter_intra_predictor_neon);
diff --git a/Source/Lib/Codec/common_dsp_rtcd.h b/Source/Lib/Codec/common_dsp_rtcd.h
index 41f7a496..7b6a73a5 100644
--- a/Source/Lib/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Codec/common_dsp_rtcd.h
@@ -1164,6 +1164,7 @@ extern "C" {
     void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_t *src0, uint32_t src0_stride, const uint8_t *src1, uint32_t src1_stride, const uint8_t *mask, uint32_t mask_stride, int w, int h, int subw, int subh);
 
     void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_t *src0, uint32_t src0_stride, const uint8_t *src1, uint32_t src1_stride, const uint8_t *mask, uint32_t mask_stride, int w, int h, int subx, int suby, int bd);
+    void svt_aom_highbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, const CONV_BUF_TYPE *src0, uint32_t src0_stride, const CONV_BUF_TYPE *src1, uint32_t src1_stride, const uint8_t *mask, uint32_t mask_stride, int w, int h, int subx, int suby, ConvolveParams *conv_params, const int bd);
 
     void svt_av1_selfguided_restoration_neon(const uint8_t *dat8, int32_t width, int32_t height, int32_t stride, int32_t *flt0, int32_t *flt1, int32_t flt_stride, int32_t sgr_params_idx, int32_t bit_depth, int32_t highbd);
 
diff --git a/test/CompoundUtilTest.cc b/test/CompoundUtilTest.cc
index 7e4a0a34..71f619f1 100644
--- a/test/CompoundUtilTest.cc
+++ b/test/CompoundUtilTest.cc
@@ -564,6 +564,13 @@ INSTANTIATE_TEST_SUITE_P(
                                     svt_aom_highbd_blend_a64_d16_mask_avx2)}));
 #endif  // ARCH_X86_64
 
+#ifdef ARCH_AARCH64
+INSTANTIATE_TEST_SUITE_P(
+    NEON, HbdCompBlendD16Test,
+    ::testing::ValuesIn({make_tuple(svt_aom_highbd_blend_a64_d16_mask_c,
+                                    svt_aom_highbd_blend_a64_d16_mask_neon)}));
+#endif  // ARCH_AARCH64
+
 using HbdBlendA64HMaskFunc = void (*)(uint8_t *, uint32_t, const uint8_t *,
                                       uint32_t, const uint8_t *, uint32_t,
                                       const uint8_t *, int, int, int);
-- 
2.36.0.windows.1

