From b8a7cf31e28a81b4a99f685bba80f5548e92b811 Mon Sep 17 00:00:00 2001
From: Salome Thirot <salome.thirot@arm.com>
Date: Mon, 28 Oct 2024 13:50:14 +0000
Subject: [PATCH 38/56] Inline and optimize helpers for Neon blend functions

Inline simple blend helpers and create widening pairwise blend helpers
to use a widening add, saving one instruction.
---
 Source/Lib/ASM_NEON/blend_a64_mask_neon.c     | 36 +++++++------------
 Source/Lib/ASM_NEON/blend_a64_mask_neon.h     | 16 ++++++---
 .../Lib/ASM_NEON/highbd_blend_a64_mask_neon.c | 26 +++++++-------
 3 files changed, 37 insertions(+), 41 deletions(-)

diff --git a/Source/Lib/ASM_NEON/blend_a64_mask_neon.c b/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
index 662c993f..e745dcdf 100644
--- a/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
+++ b/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
@@ -43,18 +43,6 @@ static INLINE uint8x8_t alpha_blend_a64_u8x8(uint8x8_t m, uint8x8_t a, uint8x8_t
     return vrshrn_n_u16(blend_u16, AOM_BLEND_A64_ROUND_BITS);
 }
 
-static INLINE uint8x16_t avg_blend_u8x16(uint8x16_t a, uint8x16_t b) { return vrhaddq_u8(a, b); }
-
-static INLINE uint8x16_t avg_blend_pairwise_u8x16(uint8x16_t a, uint8x16_t b) {
-    return vrshrq_n_u8(vpaddq_u8(a, b), 1);
-}
-
-static INLINE uint8x16_t avg_blend_pairwise_u8x16_4(uint8x16_t a, uint8x16_t b, uint8x16_t c, uint8x16_t d) {
-    uint8x16_t a_c = vpaddq_u8(a, c);
-    uint8x16_t b_d = vpaddq_u8(b, d);
-    return vrshrq_n_u8(vqaddq_u8(a_c, b_d), 2);
-}
-
 void svt_aom_blend_a64_hmask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_t *src0, uint32_t src0_stride,
                                   const uint8_t *src1, uint32_t src1_stride, const uint8_t *mask, int w, int h) {
     assert(IMPLIES(src0 == dst, src0_stride == dst_stride));
@@ -305,7 +293,7 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
                     uint16x8_t s0 = vld1q_u16(src0 + i);
                     uint16x8_t s1 = vld1q_u16(src1 + i);
 
-                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(m0, m1, m2, m3));
+                    uint16x8_t m_avg = avg_blend_pairwise_long_u8x8_4(m0, m1, m2, m3);
 
                     uint8x8_t blend = alpha_blend_a64_d16_u16x8(m_avg, s0, s1, offset_vec);
 
@@ -327,7 +315,7 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
                 uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
                 uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
 
-                uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(m0, m1, m2, m3));
+                uint16x8_t m_avg = avg_blend_pairwise_long_u8x8_4(m0, m1, m2, m3);
                 uint8x8_t  blend = alpha_blend_a64_d16_u16x8(m_avg, s0, s1, offset_vec);
 
                 store_u8x4_strided_x2(dst, dst_stride, blend);
@@ -349,7 +337,7 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
                     uint16x8_t s0 = vld1q_u16(src0 + i);
                     uint16x8_t s1 = vld1q_u16(src1 + i);
 
-                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));
+                    uint16x8_t m_avg = vmovl_u8(vrshr_n_u8(vpadd_u8(m0, m1), 1));
                     uint8x8_t  blend = alpha_blend_a64_d16_u16x8(m_avg, s0, s1, offset_vec);
 
                     vst1_u8(dst + i, blend);
@@ -368,7 +356,7 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
                 uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
                 uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
 
-                uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));
+                uint16x8_t m_avg = vmovl_u8(vrshr_n_u8(vpadd_u8(m0, m1), 1));
                 uint8x8_t  blend = alpha_blend_a64_d16_u16x8(m_avg, s0, s1, offset_vec);
 
                 store_u8x4_strided_x2(dst, dst_stride, blend);
@@ -390,7 +378,7 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
                     uint16x8_t s0 = vld1q_u16(src0 + i);
                     uint16x8_t s1 = vld1q_u16(src1 + i);
 
-                    uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0, m1));
+                    uint16x8_t m_avg = vmovl_u8(vrhadd_u8(m0, m1));
                     uint8x8_t  blend = alpha_blend_a64_d16_u16x8(m_avg, s0, s1, offset_vec);
 
                     vst1_u8(dst + i, blend);
@@ -409,7 +397,7 @@ void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, co
                 uint16x8_t s0   = load_unaligned_u16_4x2(src0, src0_stride);
                 uint16x8_t s1   = load_unaligned_u16_4x2(src1, src1_stride);
 
-                uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0_2, m1_3));
+                uint16x8_t m_avg = vmovl_u8(vrhadd_u8(m0_2, m1_3));
                 uint8x8_t  blend = alpha_blend_a64_d16_u16x8(m_avg, s0, s1, offset_vec);
 
                 store_u8x4_strided_x2(dst, dst_stride, blend);
@@ -565,7 +553,7 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
                     const uint8x16_t s0 = vld1q_u8(src0 + i);
                     const uint8x16_t s1 = vld1q_u8(src1 + i);
 
-                    const uint8x16_t m_avg = avg_blend_pairwise_u8x16(m0, m1);
+                    const uint8x16_t m_avg = vrshrq_n_u8(vpaddq_u8(m0, m1), 1);
                     const uint8x16_t blend = alpha_blend_a64_u8x16(m_avg, s0, s1);
 
                     vst1q_u8(dst + i, blend);
@@ -585,7 +573,7 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
                 const uint8x8_t s0 = vld1_u8(src0);
                 const uint8x8_t s1 = vld1_u8(src1);
 
-                const uint8x8_t m_avg = avg_blend_pairwise_u8x8(m0, m1);
+                const uint8x8_t m_avg = vrshr_n_u8(vpadd_u8(m0, m1), 1);
                 const uint8x8_t blend = alpha_blend_a64_u8x8(m_avg, s0, s1);
 
                 vst1_u8(dst, blend);
@@ -602,7 +590,7 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
                 const uint8x8_t s0 = load_unaligned_u8_4x2(src0, src0_stride);
                 const uint8x8_t s1 = load_unaligned_u8_4x2(src1, src1_stride);
 
-                const uint8x8_t m_avg = avg_blend_pairwise_u8x8(m0, m1);
+                const uint8x8_t m_avg = vrshr_n_u8(vpadd_u8(m0, m1), 1);
                 const uint8x8_t blend = alpha_blend_a64_u8x8(m_avg, s0, s1);
 
                 store_unaligned_u8_4x2(dst, dst_stride, blend);
@@ -624,7 +612,7 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
                     const uint8x16_t s0 = vld1q_u8(src0 + i);
                     const uint8x16_t s1 = vld1q_u8(src1 + i);
 
-                    const uint8x16_t m_avg = avg_blend_u8x16(m0, m1);
+                    const uint8x16_t m_avg = vrhaddq_u8(m0, m1);
                     const uint8x16_t blend = alpha_blend_a64_u8x16(m_avg, s0, s1);
 
                     vst1q_u8(dst + i, blend);
@@ -644,7 +632,7 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
                 const uint8x8_t s0 = vld1_u8(src0);
                 const uint8x8_t s1 = vld1_u8(src1);
 
-                const uint8x8_t m_avg = avg_blend_u8x8(m0, m1);
+                const uint8x8_t m_avg = vrhadd_u8(m0, m1);
                 const uint8x8_t blend = alpha_blend_a64_u8x8(m_avg, s0, s1);
 
                 vst1_u8(dst, blend);
@@ -661,7 +649,7 @@ void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_
                 const uint8x8_t s0   = load_unaligned_u8_4x2(src0, src0_stride);
                 const uint8x8_t s1   = load_unaligned_u8_4x2(src1, src1_stride);
 
-                const uint8x8_t m_avg = avg_blend_u8x8(m0_2, m1_3);
+                const uint8x8_t m_avg = vrhadd_u8(m0_2, m1_3);
                 const uint8x8_t blend = alpha_blend_a64_u8x8(m_avg, s0, s1);
 
                 store_unaligned_u8_4x2(dst, dst_stride, blend);
diff --git a/Source/Lib/ASM_NEON/blend_a64_mask_neon.h b/Source/Lib/ASM_NEON/blend_a64_mask_neon.h
index 7a939678..d4315008 100644
--- a/Source/Lib/ASM_NEON/blend_a64_mask_neon.h
+++ b/Source/Lib/ASM_NEON/blend_a64_mask_neon.h
@@ -19,14 +19,22 @@
 #include "definitions.h"
 #include "mem_neon.h"
 
-static INLINE uint8x8_t avg_blend_u8x8(uint8x8_t a, uint8x8_t b) { return vrhadd_u8(a, b); }
-
-static INLINE uint8x8_t avg_blend_pairwise_u8x8(uint8x8_t a, uint8x8_t b) { return vrshr_n_u8(vpadd_u8(a, b), 1); }
-
 static INLINE uint8x8_t avg_blend_pairwise_u8x8_4(uint8x8_t a, uint8x8_t b, uint8x8_t c, uint8x8_t d) {
     uint8x8_t a_c = vpadd_u8(a, c);
     uint8x8_t b_d = vpadd_u8(b, d);
     return vrshr_n_u8(vqadd_u8(a_c, b_d), 2);
 }
 
+static INLINE uint16x8_t avg_blend_pairwise_long_u8x8_4(uint8x8_t a, uint8x8_t b, uint8x8_t c, uint8x8_t d) {
+    uint8x8_t a_c = vpadd_u8(a, c);
+    uint8x8_t b_d = vpadd_u8(b, d);
+    return vrshrq_n_u16(vaddl_u8(a_c, b_d), 2);
+}
+
+static INLINE uint8x16_t avg_blend_pairwise_u8x16_4(uint8x16_t a, uint8x16_t b, uint8x16_t c, uint8x16_t d) {
+    uint8x16_t a_c = vpaddq_u8(a, c);
+    uint8x16_t b_d = vpaddq_u8(b, d);
+    return vrshrq_n_u8(vqaddq_u8(a_c, b_d), 2);
+}
+
 #endif // BLEND_A64_MASK_NEON_H
diff --git a/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c b/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
index 0bcdb749..9588770d 100644
--- a/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
@@ -101,7 +101,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                     uint16x8_t s0 = vld1q_u16(src0 + i);
                     uint16x8_t s1 = vld1q_u16(src1 + i);
 
-                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(m0, m1, m2, m3));
+                    uint16x8_t m_avg = avg_blend_pairwise_long_u8x8_4(m0, m1, m2, m3);
 
                     uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
 
@@ -124,7 +124,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                 uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
                 uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
 
-                uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(m0, m1, m2, m3));
+                uint16x8_t m_avg = avg_blend_pairwise_long_u8x8_4(m0, m1, m2, m3);
                 uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
 
                 store_u16x4_strided_x2(dst, dst_stride, blend);
@@ -147,7 +147,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                     uint16x8_t s0 = vld1q_u16(src0 + i);
                     uint16x8_t s1 = vld1q_u16(src1 + i);
 
-                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));
+                    uint16x8_t m_avg = vmovl_u8(vrshr_n_u8(vpadd_u8(m0, m1), 1));
                     uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
 
                     vst1q_u16(dst + i, blend);
@@ -167,7 +167,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                 uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
                 uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
 
-                uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));
+                uint16x8_t m_avg = vmovl_u8(vrshr_n_u8(vpadd_u8(m0, m1), 1));
                 uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
 
                 store_u16x4_strided_x2(dst, dst_stride, blend);
@@ -189,7 +189,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                     uint16x8_t s0 = vld1q_u16(src0 + i);
                     uint16x8_t s1 = vld1q_u16(src1 + i);
 
-                    uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0, m1));
+                    uint16x8_t m_avg = vmovl_u8(vrhadd_u8(m0, m1));
                     uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
 
                     vst1q_u16(dst + i, blend);
@@ -209,7 +209,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                 uint16x8_t s0   = load_unaligned_u16_4x2(src0, src0_stride);
                 uint16x8_t s1   = load_unaligned_u16_4x2(src1, src1_stride);
 
-                uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0_2, m1_3));
+                uint16x8_t m_avg = vmovl_u8(vrhadd_u8(m0_2, m1_3));
                 uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
 
                 store_u16x4_strided_x2(dst, dst_stride, blend);
@@ -312,8 +312,8 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                         uint16x8_t s0 = vld1q_u16(src0 + i);                                                          \
                         uint16x8_t s1 = vld1q_u16(src1 + i);                                                          \
                                                                                                                       \
-                        uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(                                        \
-                            vget_low_u8(m0), vget_low_u8(m1), vget_high_u8(m0), vget_high_u8(m1)));                   \
+                        uint16x8_t m_avg = avg_blend_pairwise_long_u8x8_4(                                            \
+                            vget_low_u8(m0), vget_low_u8(m1), vget_high_u8(m0), vget_high_u8(m1));                    \
                         uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                   \
                                                                                                                       \
                         vst1q_u16(dst + i, blend);                                                                    \
@@ -334,7 +334,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                     uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);                                        \
                     uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);                                        \
                                                                                                                       \
-                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(m0, m1, m2, m3));                           \
+                    uint16x8_t m_avg = avg_blend_pairwise_long_u8x8_4(m0, m1, m2, m3);                                \
                     uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                       \
                                                                                                                       \
                     store_u16x4_strided_x2(dst, dst_stride, blend);                                                   \
@@ -356,7 +356,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                         uint16x8_t s0 = vld1q_u16(src0 + i);                                                          \
                         uint16x8_t s1 = vld1q_u16(src1 + i);                                                          \
                                                                                                                       \
-                        uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));                                 \
+                        uint16x8_t m_avg = vmovl_u8(vrshr_n_u8(vpadd_u8(m0, m1), 1));                                 \
                         uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                   \
                                                                                                                       \
                         vst1q_u16(dst + i, blend);                                                                    \
@@ -375,7 +375,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                     uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);                                        \
                     uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);                                        \
                                                                                                                       \
-                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));                                     \
+                    uint16x8_t m_avg = vmovl_u8(vrshr_n_u8(vpadd_u8(m0, m1), 1));                                     \
                     uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                       \
                                                                                                                       \
                     store_u16x4_strided_x2(dst, dst_stride, blend);                                                   \
@@ -397,7 +397,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                         uint16x8_t s0 = vld1q_u16(src0 + i);                                                          \
                         uint16x8_t s1 = vld1q_u16(src1 + i);                                                          \
                                                                                                                       \
-                        uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0, m1));                                          \
+                        uint16x8_t m_avg = vmovl_u8(vrhadd_u8(m0, m1));                                               \
                         uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                   \
                                                                                                                       \
                         vst1q_u16(dst + i, blend);                                                                    \
@@ -416,7 +416,7 @@ void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, con
                     uint16x8_t s0   = load_unaligned_u16_4x2(src0, src0_stride);                                      \
                     uint16x8_t s1   = load_unaligned_u16_4x2(src1, src1_stride);                                      \
                                                                                                                       \
-                    uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0_2, m1_3));                                          \
+                    uint16x8_t m_avg = vmovl_u8(vrhadd_u8(m0_2, m1_3));                                               \
                     uint16x8_t blend = alpha_##bd##_blend_a64_d16_u16x8(m_avg, s0, s1, offset);                       \
                                                                                                                       \
                     store_u16x4_strided_x2(dst, dst_stride, blend);                                                   \
-- 
2.36.0.windows.1

