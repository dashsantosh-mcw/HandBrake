From 8e7713a625ccbc21c9b2918466248361805b6e79 Mon Sep 17 00:00:00 2001
From: Salome Thirot <salome.thirot@arm.com>
Date: Thu, 24 Oct 2024 16:20:10 +0100
Subject: [PATCH 33/56] Add a Neon implementation of
 svt_aom_highbd_blend_a64_mask

Port the libaom implementation of svt_aom_highbd_blend_a64_mask and add
the corresponding unit tests.
---
 Source/Lib/ASM_NEON/CMakeLists.txt            |   1 +
 Source/Lib/ASM_NEON/blend_a64_mask_neon.c     |  14 +-
 Source/Lib/ASM_NEON/blend_a64_mask_neon.h     |  32 +++
 .../Lib/ASM_NEON/highbd_blend_a64_mask_neon.c | 225 ++++++++++++++++++
 Source/Lib/ASM_NEON/mem_neon.h                |  15 ++
 Source/Lib/Codec/common_dsp_rtcd.c            |   2 +-
 Source/Lib/Codec/common_dsp_rtcd.h            |   2 +
 test/CompoundUtilTest.cc                      |   7 +
 8 files changed, 286 insertions(+), 12 deletions(-)
 create mode 100644 Source/Lib/ASM_NEON/blend_a64_mask_neon.h
 create mode 100644 Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c

diff --git a/Source/Lib/ASM_NEON/CMakeLists.txt b/Source/Lib/ASM_NEON/CMakeLists.txt
index 9e1133d5..3c438489 100644
--- a/Source/Lib/ASM_NEON/CMakeLists.txt
+++ b/Source/Lib/ASM_NEON/CMakeLists.txt
@@ -38,6 +38,7 @@ target_sources(
   PUBLIC encodetxb_neon.c
   PUBLIC transforms_intrin_neon.c
   PUBLIC hadmard_path_neon.c
+  PUBLIC highbd_blend_a64_mask_neon.c
   PUBLIC highbd_fwd_txfm_neon.c
   PUBLIC highbd_inter_prediction_neon.c
   PUBLIC highbd_intra_prediction_neon.c
diff --git a/Source/Lib/ASM_NEON/blend_a64_mask_neon.c b/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
index 9da8caa9..662c993f 100644
--- a/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
+++ b/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
@@ -11,9 +11,11 @@
 
 #include <arm_neon.h>
 #include <assert.h>
+
+#include "blend_a64_mask_neon.h"
 #include "common_dsp_rtcd.h"
-#include "mem_neon.h"
 #include "definitions.h"
+#include "mem_neon.h"
 
 static INLINE uint8x16_t alpha_blend_a64_u8x16(uint8x16_t m, uint8x16_t a, uint8x16_t b) {
     uint16x8_t       blend_u16_lo, blend_u16_hi;
@@ -41,22 +43,12 @@ static INLINE uint8x8_t alpha_blend_a64_u8x8(uint8x8_t m, uint8x8_t a, uint8x8_t
     return vrshrn_n_u16(blend_u16, AOM_BLEND_A64_ROUND_BITS);
 }
 
-static INLINE uint8x8_t avg_blend_u8x8(uint8x8_t a, uint8x8_t b) { return vrhadd_u8(a, b); }
-
 static INLINE uint8x16_t avg_blend_u8x16(uint8x16_t a, uint8x16_t b) { return vrhaddq_u8(a, b); }
 
-static INLINE uint8x8_t avg_blend_pairwise_u8x8(uint8x8_t a, uint8x8_t b) { return vrshr_n_u8(vpadd_u8(a, b), 1); }
-
 static INLINE uint8x16_t avg_blend_pairwise_u8x16(uint8x16_t a, uint8x16_t b) {
     return vrshrq_n_u8(vpaddq_u8(a, b), 1);
 }
 
-static INLINE uint8x8_t avg_blend_pairwise_u8x8_4(uint8x8_t a, uint8x8_t b, uint8x8_t c, uint8x8_t d) {
-    uint8x8_t a_c = vpadd_u8(a, c);
-    uint8x8_t b_d = vpadd_u8(b, d);
-    return vrshr_n_u8(vqadd_u8(a_c, b_d), 2);
-}
-
 static INLINE uint8x16_t avg_blend_pairwise_u8x16_4(uint8x16_t a, uint8x16_t b, uint8x16_t c, uint8x16_t d) {
     uint8x16_t a_c = vpaddq_u8(a, c);
     uint8x16_t b_d = vpaddq_u8(b, d);
diff --git a/Source/Lib/ASM_NEON/blend_a64_mask_neon.h b/Source/Lib/ASM_NEON/blend_a64_mask_neon.h
new file mode 100644
index 00000000..7a939678
--- /dev/null
+++ b/Source/Lib/ASM_NEON/blend_a64_mask_neon.h
@@ -0,0 +1,32 @@
+/*
+ * Copyright (c) 2023, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#ifndef BLEND_A64_MASK_NEON_H
+#define BLEND_A64_MASK_NEON_H
+
+#include <arm_neon.h>
+#include <assert.h>
+
+#include "common_dsp_rtcd.h"
+#include "definitions.h"
+#include "mem_neon.h"
+
+static INLINE uint8x8_t avg_blend_u8x8(uint8x8_t a, uint8x8_t b) { return vrhadd_u8(a, b); }
+
+static INLINE uint8x8_t avg_blend_pairwise_u8x8(uint8x8_t a, uint8x8_t b) { return vrshr_n_u8(vpadd_u8(a, b), 1); }
+
+static INLINE uint8x8_t avg_blend_pairwise_u8x8_4(uint8x8_t a, uint8x8_t b, uint8x8_t c, uint8x8_t d) {
+    uint8x8_t a_c = vpadd_u8(a, c);
+    uint8x8_t b_d = vpadd_u8(b, d);
+    return vrshr_n_u8(vqadd_u8(a_c, b_d), 2);
+}
+
+#endif // BLEND_A64_MASK_NEON_H
diff --git a/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c b/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
new file mode 100644
index 00000000..c68feff8
--- /dev/null
+++ b/Source/Lib/ASM_NEON/highbd_blend_a64_mask_neon.c
@@ -0,0 +1,225 @@
+/*
+ * Copyright (c) 2023, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <arm_neon.h>
+#include <assert.h>
+
+#include "blend_a64_mask_neon.h"
+#include "common_dsp_rtcd.h"
+#include "definitions.h"
+#include "mem_neon.h"
+
+static INLINE uint16x8_t alpha_blend_a64_u16x8(uint16x8_t m, uint16x8_t a, uint16x8_t b) {
+    uint16x8_t m_inv = vsubq_u16(vdupq_n_u16(AOM_BLEND_A64_MAX_ALPHA), m);
+
+    uint32x4_t blend_u32_lo = vmull_u16(vget_low_u16(a), vget_low_u16(m));
+    uint32x4_t blend_u32_hi = vmull_u16(vget_high_u16(a), vget_high_u16(m));
+
+    blend_u32_lo = vmlal_u16(blend_u32_lo, vget_low_u16(b), vget_low_u16(m_inv));
+    blend_u32_hi = vmlal_u16(blend_u32_hi, vget_high_u16(b), vget_high_u16(m_inv));
+
+    uint16x4_t blend_u16_lo = vrshrn_n_u32(blend_u32_lo, AOM_BLEND_A64_ROUND_BITS);
+    uint16x4_t blend_u16_hi = vrshrn_n_u32(blend_u32_hi, AOM_BLEND_A64_ROUND_BITS);
+
+    return vcombine_u16(blend_u16_lo, blend_u16_hi);
+}
+
+void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst_8, uint32_t dst_stride, const uint8_t *src0_8,
+                                        uint32_t src0_stride, const uint8_t *src1_8, uint32_t src1_stride,
+                                        const uint8_t *mask, uint32_t mask_stride, int w, int h, int subw, int subh,
+                                        int bd) {
+    (void)bd;
+
+    const uint16_t *src0 = (uint16_t *)src0_8;
+    const uint16_t *src1 = (uint16_t *)src1_8;
+    uint16_t       *dst  = (uint16_t *)dst_8;
+
+    assert(IMPLIES(src0 == dst, src0_stride == dst_stride));
+    assert(IMPLIES(src1 == dst, src1_stride == dst_stride));
+
+    assert(h >= 1);
+    assert(w >= 1);
+    assert(IS_POWER_OF_TWO(h));
+    assert(IS_POWER_OF_TWO(w));
+
+    assert(bd == 8 || bd == 10);
+
+    if ((subw | subh) == 0) {
+        if (w >= 8) {
+            do {
+                int i = 0;
+                do {
+                    uint16x8_t m0 = vmovl_u8(vld1_u8(mask + i));
+                    uint16x8_t s0 = vld1q_u16(src0 + i);
+                    uint16x8_t s1 = vld1q_u16(src1 + i);
+
+                    uint16x8_t blend = alpha_blend_a64_u16x8(m0, s0, s1);
+
+                    vst1q_u16(dst + i, blend);
+                    i += 8;
+                } while (i < w);
+
+                mask += mask_stride;
+                src0 += src0_stride;
+                src1 += src1_stride;
+                dst += dst_stride;
+            } while (--h != 0);
+        } else {
+            do {
+                uint16x8_t m0 = vmovl_u8(load_unaligned_u8_4x2(mask, mask_stride));
+                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+
+                uint16x8_t blend = alpha_blend_a64_u16x8(m0, s0, s1);
+
+                store_u16x4_strided_x2(dst, dst_stride, blend);
+
+                mask += 2 * mask_stride;
+                src0 += 2 * src0_stride;
+                src1 += 2 * src1_stride;
+                dst += 2 * dst_stride;
+                h -= 2;
+            } while (h != 0);
+        }
+    } else if ((subw & subh) == 1) {
+        if (w >= 8) {
+            do {
+                int i = 0;
+                do {
+                    uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride + 2 * i);
+                    uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride + 2 * i);
+                    uint8x8_t  m2 = vld1_u8(mask + 0 * mask_stride + 2 * i + 8);
+                    uint8x8_t  m3 = vld1_u8(mask + 1 * mask_stride + 2 * i + 8);
+                    uint16x8_t s0 = vld1q_u16(src0 + i);
+                    uint16x8_t s1 = vld1q_u16(src1 + i);
+
+                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(m0, m1, m2, m3));
+
+                    uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
+
+                    vst1q_u16(dst + i, blend);
+
+                    i += 8;
+                } while (i < w);
+
+                mask += 2 * mask_stride;
+                src0 += src0_stride;
+                src1 += src1_stride;
+                dst += dst_stride;
+            } while (--h != 0);
+        } else {
+            do {
+                uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride);
+                uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);
+                uint8x8_t  m2 = vld1_u8(mask + 2 * mask_stride);
+                uint8x8_t  m3 = vld1_u8(mask + 3 * mask_stride);
+                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+
+                uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8_4(m0, m1, m2, m3));
+                uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
+
+                store_u16x4_strided_x2(dst, dst_stride, blend);
+
+                mask += 4 * mask_stride;
+                src0 += 2 * src0_stride;
+                src1 += 2 * src1_stride;
+                dst += 2 * dst_stride;
+                h -= 2;
+            } while (h != 0);
+        }
+    } else if (subw == 1 && subh == 0) {
+        if (w >= 8) {
+            do {
+                int i = 0;
+
+                do {
+                    uint8x8_t  m0 = vld1_u8(mask + 2 * i);
+                    uint8x8_t  m1 = vld1_u8(mask + 2 * i + 8);
+                    uint16x8_t s0 = vld1q_u16(src0 + i);
+                    uint16x8_t s1 = vld1q_u16(src1 + i);
+
+                    uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));
+                    uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
+
+                    vst1q_u16(dst + i, blend);
+
+                    i += 8;
+                } while (i < w);
+
+                mask += mask_stride;
+                src0 += src0_stride;
+                src1 += src1_stride;
+                dst += dst_stride;
+            } while (--h != 0);
+        } else {
+            do {
+                uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride);
+                uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride);
+                uint16x8_t s0 = load_unaligned_u16_4x2(src0, src0_stride);
+                uint16x8_t s1 = load_unaligned_u16_4x2(src1, src1_stride);
+
+                uint16x8_t m_avg = vmovl_u8(avg_blend_pairwise_u8x8(m0, m1));
+                uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
+
+                store_u16x4_strided_x2(dst, dst_stride, blend);
+
+                mask += 2 * mask_stride;
+                src0 += 2 * src0_stride;
+                src1 += 2 * src1_stride;
+                dst += 2 * dst_stride;
+                h -= 2;
+            } while (h != 0);
+        }
+    } else {
+        if (w >= 8) {
+            do {
+                int i = 0;
+                do {
+                    uint8x8_t  m0 = vld1_u8(mask + 0 * mask_stride + i);
+                    uint8x8_t  m1 = vld1_u8(mask + 1 * mask_stride + i);
+                    uint16x8_t s0 = vld1q_u16(src0 + i);
+                    uint16x8_t s1 = vld1q_u16(src1 + i);
+
+                    uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0, m1));
+                    uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
+
+                    vst1q_u16(dst + i, blend);
+
+                    i += 8;
+                } while (i < w);
+
+                mask += 2 * mask_stride;
+                src0 += src0_stride;
+                src1 += src1_stride;
+                dst += dst_stride;
+            } while (--h != 0);
+        } else {
+            do {
+                uint8x8_t  m0_2 = load_unaligned_u8_4x2(mask + 0 * mask_stride, 2 * mask_stride);
+                uint8x8_t  m1_3 = load_unaligned_u8_4x2(mask + 1 * mask_stride, 2 * mask_stride);
+                uint16x8_t s0   = load_unaligned_u16_4x2(src0, src0_stride);
+                uint16x8_t s1   = load_unaligned_u16_4x2(src1, src1_stride);
+
+                uint16x8_t m_avg = vmovl_u8(avg_blend_u8x8(m0_2, m1_3));
+                uint16x8_t blend = alpha_blend_a64_u16x8(m_avg, s0, s1);
+
+                store_u16x4_strided_x2(dst, dst_stride, blend);
+
+                mask += 4 * mask_stride;
+                src0 += 2 * src0_stride;
+                src1 += 2 * src1_stride;
+                dst += 2 * dst_stride;
+                h -= 2;
+            } while (h != 0);
+        }
+    }
+}
diff --git a/Source/Lib/ASM_NEON/mem_neon.h b/Source/Lib/ASM_NEON/mem_neon.h
index 1dff30af..09eaae9a 100644
--- a/Source/Lib/ASM_NEON/mem_neon.h
+++ b/Source/Lib/ASM_NEON/mem_neon.h
@@ -1184,6 +1184,12 @@ static inline void store_u8x2_strided_x2(uint8_t *dst, uint32_t dst_stride, uint
 #undef store_u8_4x1_lane
 #undef store_u8_2x1_lane
 
+#define store_u16_4x1_lane(dst, src, lane)                             \
+    do {                                                               \
+        uint64_t a = vgetq_lane_u64(vreinterpretq_u64_u16(src), lane); \
+        memcpy(dst, &a, 8);                                            \
+    } while (0)
+
 #define store_s16_4x1_lane(dst, src, lane)                            \
     do {                                                              \
         int64_t a = vgetq_lane_s64(vreinterpretq_s64_s16(src), lane); \
@@ -1196,6 +1202,15 @@ static INLINE void store_s16x4_strided_x2(int16_t *dst, int32_t dst_stride, int1
     dst += dst_stride;
     store_s16_4x1_lane(dst, src, 1);
 }
+
+// Store two blocks of 64-bits from a single vector.
+static INLINE void store_u16x4_strided_x2(uint16_t *dst, uint32_t dst_stride, uint16x8_t src) {
+    store_u16_4x1_lane(dst, src, 0);
+    dst += dst_stride;
+    store_u16_4x1_lane(dst, src, 1);
+}
+
+#undef store_u16_4x1_lane
 #undef store_s16_4x1_lane
 
 #endif // AOM_AOM_DSP_ARM_MEM_NEON_H_
diff --git a/Source/Lib/Codec/common_dsp_rtcd.c b/Source/Lib/Codec/common_dsp_rtcd.c
index 79610f4a..5739bf33 100644
--- a/Source/Lib/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Codec/common_dsp_rtcd.c
@@ -1000,7 +1000,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_aom_blend_a64_hmask, svt_aom_blend_a64_hmask_c, svt_aom_blend_a64_hmask_neon);
     SET_NEON(svt_aom_blend_a64_vmask, svt_aom_blend_a64_vmask_c, svt_aom_blend_a64_vmask_neon);
     SET_NEON(svt_aom_lowbd_blend_a64_d16_mask, svt_aom_lowbd_blend_a64_d16_mask_c, svt_aom_lowbd_blend_a64_d16_mask_neon);
-    SET_ONLY_C(svt_aom_highbd_blend_a64_mask, svt_aom_highbd_blend_a64_mask_c);
+    SET_NEON(svt_aom_highbd_blend_a64_mask, svt_aom_highbd_blend_a64_mask_c, svt_aom_highbd_blend_a64_mask_neon);
     SET_ONLY_C(svt_aom_highbd_blend_a64_hmask_8bit, svt_aom_highbd_blend_a64_hmask_8bit_c);
     SET_ONLY_C(svt_aom_highbd_blend_a64_vmask_8bit, svt_aom_highbd_blend_a64_vmask_8bit_c);
     SET_ONLY_C(svt_aom_highbd_blend_a64_vmask_16bit, svt_aom_highbd_blend_a64_vmask_16bit_c);
diff --git a/Source/Lib/Codec/common_dsp_rtcd.h b/Source/Lib/Codec/common_dsp_rtcd.h
index 4850017a..41f7a496 100644
--- a/Source/Lib/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Codec/common_dsp_rtcd.h
@@ -1163,6 +1163,8 @@ extern "C" {
     void svt_aom_lowbd_blend_a64_d16_mask_neon(uint8_t *dst, uint32_t dst_stride, const CONV_BUF_TYPE *src0, uint32_t src0_stride, const CONV_BUF_TYPE *src1, uint32_t src1_stride, const uint8_t *mask, uint32_t mask_stride, int w, int h, int subw, int subh, ConvolveParams *conv_params);
     void svt_aom_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_t *src0, uint32_t src0_stride, const uint8_t *src1, uint32_t src1_stride, const uint8_t *mask, uint32_t mask_stride, int w, int h, int subw, int subh);
 
+    void svt_aom_highbd_blend_a64_mask_neon(uint8_t *dst, uint32_t dst_stride, const uint8_t *src0, uint32_t src0_stride, const uint8_t *src1, uint32_t src1_stride, const uint8_t *mask, uint32_t mask_stride, int w, int h, int subx, int suby, int bd);
+
     void svt_av1_selfguided_restoration_neon(const uint8_t *dat8, int32_t width, int32_t height, int32_t stride, int32_t *flt0, int32_t *flt1, int32_t flt_stride, int32_t sgr_params_idx, int32_t bit_depth, int32_t highbd);
 
     void svt_av1_filter_intra_edge_neon(uint8_t *p, int32_t sz, int32_t strength);
diff --git a/test/CompoundUtilTest.cc b/test/CompoundUtilTest.cc
index af45e0a8..7e4a0a34 100644
--- a/test/CompoundUtilTest.cc
+++ b/test/CompoundUtilTest.cc
@@ -483,6 +483,13 @@ INSTANTIATE_TEST_SUITE_P(SSE4_1, HbdCompBlendTest,
                              svt_aom_highbd_blend_a64_mask_8bit_sse4_1)}));
 #endif  // ARCH_X86_64
 
+#ifdef ARCH_AARCH64
+INSTANTIATE_TEST_SUITE_P(
+    NEON, HbdCompBlendTest,
+    ::testing::ValuesIn({make_tuple(svt_aom_highbd_blend_a64_mask_c,
+                                    svt_aom_highbd_blend_a64_mask_neon)}));
+#endif  // ARCH_AARCH64
+
 using HbdBlendA64D16MaskFunc = void (*)(uint8_t *, uint32_t,
                                         const CONV_BUF_TYPE *, uint32_t,
                                         const CONV_BUF_TYPE *, uint32_t,
-- 
2.36.0.windows.1

