From fde756790c54adab4fca7154c68df03bba3b4138 Mon Sep 17 00:00:00 2001
From: Salome Thirot <salome.thirot@arm.com>
Date: Thu, 24 Oct 2024 14:49:35 +0100
Subject: [PATCH 30/56] Optimize svt_aom_blend_a64_hmask_neon

Port the libaom implementation of svt_aom_blend_a64_hmask_neon, which is
a reduction of 20-30% in cycle count.
---
 Source/Lib/ASM_NEON/blend_a64_mask_neon.c | 139 +++++++++-------------
 Source/Lib/ASM_NEON/mem_neon.h            |  16 +++
 2 files changed, 74 insertions(+), 81 deletions(-)

diff --git a/Source/Lib/ASM_NEON/blend_a64_mask_neon.c b/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
index 880058c4..586c552c 100644
--- a/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
+++ b/Source/Lib/ASM_NEON/blend_a64_mask_neon.c
@@ -72,94 +72,71 @@ void svt_aom_blend_a64_hmask_neon(uint8_t *dst, uint32_t dst_stride, const uint8
     assert(w >= 2);
     assert(IS_POWER_OF_TWO(h));
     assert(IS_POWER_OF_TWO(w));
-    uint8x8_t       tmp0, tmp1;
-    uint8x16_t      res_q;
-    uint16x8_t      res, res_low, res_high;
-    int             i, j;
-    const uint8x8_t vdup_64 = vdup_n_u8((uint8_t)64);
 
     if (w >= 16) {
-        const uint8x16_t vdup_64_q = vdupq_n_u8((uint8_t)64);
-        for (i = 0; i < h; ++i) {
-            for (j = 0; j < w; j += 16) {
-                __builtin_prefetch(src0);
-                __builtin_prefetch(src1);
-                const uint8x16_t tmp0_q        = vld1q_u8(src0);
-                const uint8x16_t tmp1_q        = vld1q_u8(src1);
-                const uint8x16_t m_q           = vld1q_u8(mask);
-                const uint8x16_t max_minus_m_q = vsubq_u8(vdup_64_q, m_q);
-                res_low                        = vmull_u8(vget_low_u8(m_q), vget_low_u8(tmp0_q));
-                res_low                        = vmlal_u8(res_low, vget_low_u8(max_minus_m_q), vget_low_u8(tmp1_q));
-                res_high                       = vmull_u8(vget_high_u8(m_q), vget_high_u8(tmp0_q));
-                res_high                       = vmlal_u8(res_high, vget_high_u8(max_minus_m_q), vget_high_u8(tmp1_q));
-                res_q                          = vcombine_u8(vrshrn_n_u16(res_low, AOM_BLEND_A64_ROUND_BITS),
-                                    vrshrn_n_u16(res_high, AOM_BLEND_A64_ROUND_BITS));
-                vst1q_u8(dst, res_q);
-                src0 += 16;
-                src1 += 16;
-                dst += 16;
-                mask += 16;
-            }
-            src0 += src0_stride - w;
-            src1 += src1_stride - w;
-            dst += dst_stride - w;
-            mask -= w;
-        }
+        do {
+            int i = 0;
+            do {
+                uint8x16_t m0 = vld1q_u8(mask + i);
+                uint8x16_t s0 = vld1q_u8(src0 + i);
+                uint8x16_t s1 = vld1q_u8(src1 + i);
+
+                uint8x16_t blend = alpha_blend_a64_u8x16(m0, s0, s1);
+
+                vst1q_u8(dst + i, blend);
+
+                i += 16;
+            } while (i != w);
+
+            src0 += src0_stride;
+            src1 += src1_stride;
+            dst += dst_stride;
+        } while (--h != 0);
     } else if (w == 8) {
-        const uint8x8_t m           = vld1_u8(mask);
-        const uint8x8_t max_minus_m = vsub_u8(vdup_64, m);
-        for (i = 0; i < h; ++i) {
-            __builtin_prefetch(src0);
-            __builtin_prefetch(src1);
-            tmp0 = vld1_u8(src0);
-            tmp1 = vld1_u8(src1);
-            res  = vmull_u8(m, tmp0);
-            res  = vmlal_u8(res, max_minus_m, tmp1);
-            vst1_u8(dst, vrshrn_n_u16(res, AOM_BLEND_A64_ROUND_BITS));
+        const uint8x8_t m0 = vld1_u8(mask);
+        do {
+            uint8x8_t s0 = vld1_u8(src0);
+            uint8x8_t s1 = vld1_u8(src1);
+
+            uint8x8_t blend = alpha_blend_a64_u8x8(m0, s0, s1);
+
+            vst1_u8(dst, blend);
+
             src0 += src0_stride;
             src1 += src1_stride;
             dst += dst_stride;
-        }
+        } while (--h != 0);
     } else if (w == 4) {
-        assert(((uintptr_t)mask & 3) == 0);
-        const uint8x8_t m           = vreinterpret_u8_u32(vld1_dup_u32((uint32_t *)mask));
-        const uint8x8_t max_minus_m = vsub_u8(vdup_64, m);
-        for (i = 0; i < h; i += 2) {
-            __builtin_prefetch(src0 + 0 * src0_stride);
-            __builtin_prefetch(src0 + 1 * src0_stride);
-            __builtin_prefetch(src1 + 0 * src1_stride);
-            __builtin_prefetch(src1 + 1 * src1_stride);
-            tmp0                   = load_unaligned_u8_4x2(src0, src0_stride);
-            tmp1                   = load_unaligned_u8_4x2(src1, src1_stride);
-            res                    = vmull_u8(m, tmp0);
-            res                    = vmlal_u8(res, max_minus_m, tmp1);
-            const uint8x8_t result = vrshrn_n_u16(res, AOM_BLEND_A64_ROUND_BITS);
-            store_unaligned_u8_4x1(dst + 0 * dst_stride, result, 0);
-            store_unaligned_u8_4x1(dst + 1 * dst_stride, result, 1);
-            src0 += (2 * src0_stride);
-            src1 += (2 * src1_stride);
-            dst += (2 * dst_stride);
-        }
-    } else if (w == 2) {
-        assert(((uintptr_t)mask & 1) == 0);
-        const uint8x8_t m           = vreinterpret_u8_u16(vld1_dup_u16((uint16_t *)mask));
-        const uint8x8_t max_minus_m = vsub_u8(vdup_64, m);
-        for (i = 0; i < h; i += 2) {
-            __builtin_prefetch(src0 + 0 * src0_stride);
-            __builtin_prefetch(src0 + 1 * src0_stride);
-            __builtin_prefetch(src1 + 0 * src1_stride);
-            __builtin_prefetch(src1 + 1 * src1_stride);
-            tmp0                   = load_unaligned_u8_2x2(src0, src0_stride);
-            tmp1                   = load_unaligned_u8_2x2(src1, src1_stride);
-            res                    = vmull_u8(m, tmp0);
-            res                    = vmlal_u8(res, max_minus_m, tmp1);
-            const uint8x8_t result = vrshrn_n_u16(res, AOM_BLEND_A64_ROUND_BITS);
-            store_unaligned_u8_2x1(dst + 0 * dst_stride, result, 0);
-            store_unaligned_u8_2x1(dst + 1 * dst_stride, result, 1);
-            src0 += (2 * src0_stride);
-            src1 += (2 * src1_stride);
-            dst += (2 * dst_stride);
-        }
+        const uint8x8_t m0 = load_unaligned_dup_u8_4x2(mask);
+        do {
+            uint8x8_t s0 = load_unaligned_u8_4x2(src0, src0_stride);
+            uint8x8_t s1 = load_unaligned_u8_4x2(src1, src1_stride);
+
+            uint8x8_t blend = alpha_blend_a64_u8x8(m0, s0, s1);
+
+            store_u8x4_strided_x2(dst, dst_stride, blend);
+
+            src0 += 2 * src0_stride;
+            src1 += 2 * src1_stride;
+            dst += 2 * dst_stride;
+            h -= 2;
+        } while (h != 0);
+    } else {
+        assert(w == 2);
+        const uint8x8_t m0 = vreinterpret_u8_u16(vld1_dup_u16((uint16_t *)mask));
+        do {
+            uint8x8_t s0 = load_unaligned_u8_2x2(src0, src0_stride);
+            uint8x8_t s1 = load_unaligned_u8_2x2(src1, src1_stride);
+
+            uint8x8_t blend = alpha_blend_a64_u8x8(m0, s0, s1);
+
+            store_u8x2_strided_x2(dst, dst_stride, blend);
+
+            src0 += 2 * src0_stride;
+            src1 += 2 * src1_stride;
+            dst += 2 * dst_stride;
+            h -= 2;
+        } while (h != 0);
     }
 }
 
diff --git a/Source/Lib/ASM_NEON/mem_neon.h b/Source/Lib/ASM_NEON/mem_neon.h
index 05e1acea..1dff30af 100644
--- a/Source/Lib/ASM_NEON/mem_neon.h
+++ b/Source/Lib/ASM_NEON/mem_neon.h
@@ -1154,6 +1154,13 @@ static INLINE void store_unaligned_u16_4x2(uint16_t *dst, uint32_t dst_stride, u
     store_unaligned_u16_4x1(dst, src, 1);
 }
 
+// The `lane` parameter here must be an immediate.
+#define store_u8_2x1_lane(dst, src, lane)                           \
+    do {                                                            \
+        uint16_t a = vget_lane_u16(vreinterpret_u16_u8(src), lane); \
+        memcpy(dst, &a, 2);                                         \
+    } while (0)
+
 #define store_u8_4x1_lane(dst, src, lane)                           \
     do {                                                            \
         uint32_t a = vget_lane_u32(vreinterpret_u32_u8(src), lane); \
@@ -1166,7 +1173,16 @@ static INLINE void store_u8x4_strided_x2(uint8_t *dst, ptrdiff_t stride, uint8x8
     dst += stride;
     store_u8_4x1_lane(dst, src, 1);
 }
+
+// Store two blocks of 16-bits from a single vector.
+static inline void store_u8x2_strided_x2(uint8_t *dst, uint32_t dst_stride, uint8x8_t src) {
+    store_u8_2x1_lane(dst, src, 0);
+    dst += dst_stride;
+    store_u8_2x1_lane(dst, src, 1);
+}
+
 #undef store_u8_4x1_lane
+#undef store_u8_2x1_lane
 
 #define store_s16_4x1_lane(dst, src, lane)                            \
     do {                                                              \
-- 
2.36.0.windows.1

