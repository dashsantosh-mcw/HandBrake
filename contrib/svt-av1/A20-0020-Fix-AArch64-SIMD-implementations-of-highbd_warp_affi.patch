From 2f310724f5fa71533fff7ab6ab002695f504a5c4 Mon Sep 17 00:00:00 2001
From: Salome Thirot <salome.thirot@arm.com>
Date: Wed, 6 Nov 2024 12:28:19 +0000
Subject: [PATCH 20/56] Fix AArch64 SIMD implementations of highbd_warp_affine

Compute the 16-bit values directly whenever the ref buffer is needed
rather than pre-computing the whole buffer beforehand, as this was
responsible for a significant regression for 10-bit encoding.

Change-Id: I58b6b55b1a234ef96beba30889a2742580067b54
---
 Source/Lib/ASM_NEON/highbd_warp_plane_neon.c |   9 +-
 Source/Lib/ASM_NEON/highbd_warp_plane_neon.h | 236 +++++++++++--------
 Source/Lib/ASM_SVE/highbd_warp_plane_sve.c   |   9 +-
 3 files changed, 151 insertions(+), 103 deletions(-)

diff --git a/Source/Lib/ASM_NEON/highbd_warp_plane_neon.c b/Source/Lib/ASM_NEON/highbd_warp_plane_neon.c
index e26f548a..0a9b344b 100644
--- a/Source/Lib/ASM_NEON/highbd_warp_plane_neon.c
+++ b/Source/Lib/ASM_NEON/highbd_warp_plane_neon.c
@@ -254,13 +254,13 @@ void svt_av1_highbd_warp_affine_neon(const int32_t *mat, const uint8_t *ref8b, c
                                      int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y,
                                      int bd, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma,
                                      int16_t delta) {
-    uint16_t *ref = svt_aom_memalign(32, sizeof(uint16_t) * width * height);
-    svt_enc_msb_pack2d_neon((uint8_t *)ref8b, stride8b, (uint8_t *)ref2b, ref, stride2b, width, width, height);
     highbd_warp_affine_common(mat,
-                              ref,
+                              ref8b,
+                              ref2b,
                               width,
                               height,
-                              width,
+                              stride8b,
+                              stride2b,
                               pred,
                               p_col,
                               p_row,
@@ -275,5 +275,4 @@ void svt_av1_highbd_warp_affine_neon(const int32_t *mat, const uint8_t *ref8b, c
                               beta,
                               gamma,
                               delta);
-    svt_aom_free(ref);
 }
diff --git a/Source/Lib/ASM_NEON/highbd_warp_plane_neon.h b/Source/Lib/ASM_NEON/highbd_warp_plane_neon.h
index 99575524..46013a5e 100644
--- a/Source/Lib/ASM_NEON/highbd_warp_plane_neon.h
+++ b/Source/Lib/ASM_NEON/highbd_warp_plane_neon.h
@@ -90,12 +90,13 @@ static AOM_FORCE_INLINE uint16x4_t clip_pixel_highbd_vec(int32x4_t val, int bd)
 }
 
 static AOM_FORCE_INLINE uint16x8x2_t clamp_horizontal(uint16x8x2_t src_1, int out_of_boundary_left,
-                                                      int out_of_boundary_right, const uint16_t *ref, int iy,
-                                                      int stride, int width, const uint16x8_t indx0,
-                                                      const uint16x8_t indx1) {
+                                                      int out_of_boundary_right, const uint8_t *ref2b,
+                                                      const uint8_t *ref8b, int iy, int stride2b, int stride8b,
+                                                      int width, const uint16x8_t indx0, const uint16x8_t indx1) {
     if (out_of_boundary_left >= 0) {
         uint16x8_t cmp_vec = vdupq_n_u16(out_of_boundary_left);
-        uint16x8_t vec_dup = vdupq_n_u16(ref[iy * stride]);
+        uint16_t   ref     = (ref8b[iy * stride8b] << 2) | ((ref2b[iy * stride2b] >> 6) & 3);
+        uint16x8_t vec_dup = vdupq_n_u16(ref);
         uint16x8_t mask0   = vcleq_u16(indx0, cmp_vec);
         uint16x8_t mask1   = vcleq_u16(indx1, cmp_vec);
         src_1.val[0]       = vbslq_u16(mask0, vec_dup, src_1.val[0]);
@@ -103,7 +104,8 @@ static AOM_FORCE_INLINE uint16x8x2_t clamp_horizontal(uint16x8x2_t src_1, int ou
     }
     if (out_of_boundary_right >= 0) {
         uint16x8_t cmp_vec = vdupq_n_u16(15 - out_of_boundary_right);
-        uint16x8_t vec_dup = vdupq_n_u16(ref[iy * stride + width - 1]);
+        uint16_t   ref     = (ref8b[iy * stride8b + width - 1] << 2) | ((ref2b[iy * stride2b + width - 1] >> 6) & 3);
+        uint16x8_t vec_dup = vdupq_n_u16(ref);
         uint16x8_t mask0   = vcgeq_u16(indx0, cmp_vec);
         uint16x8_t mask1   = vcgeq_u16(indx1, cmp_vec);
         src_1.val[0]       = vbslq_u16(mask0, vec_dup, src_1.val[0]);
@@ -112,23 +114,23 @@ static AOM_FORCE_INLINE uint16x8x2_t clamp_horizontal(uint16x8x2_t src_1, int ou
     return src_1;
 }
 
-static AOM_FORCE_INLINE void warp_affine_horizontal(const uint16_t *ref, int width, int height, int stride, int p_width,
-                                                    int16_t alpha, int16_t beta, int iy4, int sx4, int ix4,
-                                                    int16x8_t tmp[], int bd) {
+static AOM_FORCE_INLINE void warp_affine_horizontal(const uint8_t *ref8b, const uint8_t *ref2b, int width, int height,
+                                                    int stride8b, int stride2b, int p_width, int16_t alpha,
+                                                    int16_t beta, int iy4, int sx4, int ix4, int16x8_t tmp[], int bd) {
     if (ix4 <= -7) {
         for (int k = 0; k < 15; ++k) {
-            int     iy      = clamp(iy4 + k - 7, 0, height - 1);
-            int32_t dup_val = (1 << (bd + FILTER_BITS - ROUND0_BITS - 1)) +
-                ref[iy * stride] * (1 << (FILTER_BITS - ROUND0_BITS));
-            tmp[k] = vdupq_n_s16(dup_val);
+            int      iy      = clamp(iy4 + k - 7, 0, height - 1);
+            uint16_t ref     = (ref8b[iy * stride8b] << 2) | ((ref2b[iy * stride2b] >> 6) & 3);
+            int32_t  dup_val = (1 << (bd + FILTER_BITS - ROUND0_BITS - 1)) + ref * (1 << (FILTER_BITS - ROUND0_BITS));
+            tmp[k]           = vdupq_n_s16(dup_val);
         }
         return;
     } else if (ix4 >= width + 6) {
         for (int k = 0; k < 15; ++k) {
-            int     iy      = clamp(iy4 + k - 7, 0, height - 1);
-            int32_t dup_val = (1 << (bd + FILTER_BITS - ROUND0_BITS - 1)) +
-                ref[iy * stride + (width - 1)] * (1 << (FILTER_BITS - ROUND0_BITS));
-            tmp[k] = vdupq_n_s16(dup_val);
+            int      iy  = clamp(iy4 + k - 7, 0, height - 1);
+            uint16_t ref = (ref8b[iy * stride8b + (width - 1)] << 2) | ((ref2b[iy * stride2b + (width - 1)] >> 6) & 3);
+            int32_t  dup_val = (1 << (bd + FILTER_BITS - ROUND0_BITS - 1)) + ref * (1 << (FILTER_BITS - ROUND0_BITS));
+            tmp[k]           = vdupq_n_s16(dup_val);
         }
         return;
     }
@@ -140,80 +142,126 @@ static AOM_FORCE_INLINE void warp_affine_horizontal(const uint16_t *ref, int wid
     const int out_of_boundary_left  = -(ix4 - 6);
     const int out_of_boundary_right = (ix4 + 8) - width;
 
-#define APPLY_HORIZONTAL_SHIFT_4X1(fn, ...)                                                                                    \
-    do {                                                                                                                       \
-        if (out_of_boundary_left >= 0 || out_of_boundary_right >= 0) {                                                         \
-            for (int k = 0; k < 15; ++k) {                                                                                     \
-                const int       iy    = clamp(iy4 + k - 7, 0, height - 1);                                                     \
-                const uint16_t *idx   = ref + iy * stride + ix4 - 7;                                                           \
-                uint16x8x2_t    src_1 = vld1q_u16_x2(idx);                                                                     \
-                src_1                 = clamp_horizontal(                                                                      \
-                    src_1, out_of_boundary_left, out_of_boundary_right, ref, iy, stride, width, indx0, indx1); \
-                int16x8_t rv0 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 0);                              \
-                int16x8_t rv1 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 1);                              \
-                int16x8_t rv2 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 2);                              \
-                int16x8_t rv3 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 3);                              \
-                tmp[k] = (fn)(rv0, rv1, rv2, rv3, __VA_ARGS__);                                                                \
-            }                                                                                                                  \
-        } else {                                                                                                               \
-            for (int k = 0; k < 15; ++k) {                                                                                     \
-                const int       iy  = clamp(iy4 + k - 7, 0, height - 1);                                                       \
-                const uint16_t *src = ref + iy * stride + ix4;                                                                 \
-                int16x8_t       rv0 = vreinterpretq_s16_u16(vld1q_u16(src - 7));                                               \
-                int16x8_t       rv1 = vreinterpretq_s16_u16(vld1q_u16(src - 6));                                               \
-                int16x8_t       rv2 = vreinterpretq_s16_u16(vld1q_u16(src - 5));                                               \
-                int16x8_t       rv3 = vreinterpretq_s16_u16(vld1q_u16(src - 4));                                               \
-                tmp[k]              = (fn)(rv0, rv1, rv2, rv3, __VA_ARGS__);                                                   \
-            }                                                                                                                  \
-        }                                                                                                                      \
+#define APPLY_HORIZONTAL_SHIFT_4X1(fn, ...)                                                            \
+    do {                                                                                               \
+        if (out_of_boundary_left >= 0 || out_of_boundary_right >= 0) {                                 \
+            for (int k = 0; k < 15; ++k) {                                                             \
+                const int      iy    = clamp(iy4 + k - 7, 0, height - 1);                              \
+                const uint8_t *idx2b = ref2b + iy * stride2b + ix4 - 7;                                \
+                const uint8_t *idx8b = ref8b + iy * stride8b + ix4 - 7;                                \
+                                                                                                       \
+                uint8x16_t   src_1_2b = vld1q_u8(idx2b);                                               \
+                uint8x16_t   src_1_8b = vld1q_u8(idx8b);                                               \
+                uint16x8x2_t src_1;                                                                    \
+                src_1.val[0] = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(src_1_2b, src_1_8b)), 6);    \
+                src_1.val[1] = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(src_1_2b, src_1_8b)), 6);    \
+                                                                                                       \
+                src_1         = clamp_horizontal(src_1,                                                \
+                                         out_of_boundary_left,                                 \
+                                         out_of_boundary_right,                                \
+                                         ref2b,                                                \
+                                         ref8b,                                                \
+                                         iy,                                                   \
+                                         stride2b,                                             \
+                                         stride8b,                                             \
+                                         width,                                                \
+                                         indx0,                                                \
+                                         indx1);                                               \
+                int16x8_t rv0 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 0);      \
+                int16x8_t rv1 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 1);      \
+                int16x8_t rv2 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 2);      \
+                int16x8_t rv3 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 3);      \
+                tmp[k] = (fn)(rv0, rv1, rv2, rv3, __VA_ARGS__);                                        \
+            }                                                                                          \
+        } else {                                                                                       \
+            for (int k = 0; k < 15; ++k) {                                                             \
+                const int iy = clamp(iy4 + k - 7, 0, height - 1);                                      \
+                                                                                                       \
+                const uint8_t *idx2b  = ref2b + iy * stride2b + ix4 - 7;                               \
+                const uint8_t *idx8b  = ref8b + iy * stride8b + ix4 - 7;                               \
+                uint8x16_t     src2b  = vld1q_u8(idx2b);                                               \
+                uint8x16_t     src8b  = vld1q_u8(idx8b);                                               \
+                uint16x8_t     src_lo = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(src2b, src8b)), 6); \
+                uint16x8_t     src_hi = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(src2b, src8b)), 6); \
+                                                                                                       \
+                int16x8_t rv0 = vreinterpretq_s16_u16(src_lo);                                         \
+                int16x8_t rv1 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 1));                   \
+                int16x8_t rv2 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 2));                   \
+                int16x8_t rv3 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 3));                   \
+                tmp[k]        = (fn)(rv0, rv1, rv2, rv3, __VA_ARGS__);                                 \
+            }                                                                                          \
+        }                                                                                              \
     } while (0)
 
-#define APPLY_HORIZONTAL_SHIFT_8X1(fn, ...)                                                                                    \
-    do {                                                                                                                       \
-        if (out_of_boundary_left >= 0 || out_of_boundary_right >= 0) {                                                         \
-            for (int k = 0; k < 15; ++k) {                                                                                     \
-                const int       iy    = clamp(iy4 + k - 7, 0, height - 1);                                                     \
-                const uint16_t *idx   = ref + iy * stride + ix4 - 7;                                                           \
-                uint16x8x2_t    src_1 = vld1q_u16_x2(idx);                                                                     \
-                src_1                 = clamp_horizontal(                                                                      \
-                    src_1, out_of_boundary_left, out_of_boundary_right, ref, iy, stride, width, indx0, indx1); \
-                int16x8_t rv0 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 0);                              \
-                int16x8_t rv1 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 1);                              \
-                int16x8_t rv2 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 2);                              \
-                int16x8_t rv3 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 3);                              \
-                int16x8_t rv4 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 4);                              \
-                int16x8_t rv5 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 5);                              \
-                int16x8_t rv6 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 6);                              \
-                int16x8_t rv7 = vextq_s16(                                                                                     \
-                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 7);                              \
-                tmp[k] = (fn)(rv0, rv1, rv2, rv3, rv4, rv5, rv6, rv7, __VA_ARGS__);                                            \
-            }                                                                                                                  \
-        } else {                                                                                                               \
-            for (int k = 0; k < 15; ++k) {                                                                                     \
-                const int       iy  = clamp(iy4 + k - 7, 0, height - 1);                                                       \
-                const uint16_t *src = ref + iy * stride + ix4;                                                                 \
-                int16x8_t       rv0 = vreinterpretq_s16_u16(vld1q_u16(src - 7));                                               \
-                int16x8_t       rv1 = vreinterpretq_s16_u16(vld1q_u16(src - 6));                                               \
-                int16x8_t       rv2 = vreinterpretq_s16_u16(vld1q_u16(src - 5));                                               \
-                int16x8_t       rv3 = vreinterpretq_s16_u16(vld1q_u16(src - 4));                                               \
-                int16x8_t       rv4 = vreinterpretq_s16_u16(vld1q_u16(src - 3));                                               \
-                int16x8_t       rv5 = vreinterpretq_s16_u16(vld1q_u16(src - 2));                                               \
-                int16x8_t       rv6 = vreinterpretq_s16_u16(vld1q_u16(src - 1));                                               \
-                int16x8_t       rv7 = vreinterpretq_s16_u16(vld1q_u16(src - 0));                                               \
-                tmp[k]              = (fn)(rv0, rv1, rv2, rv3, rv4, rv5, rv6, rv7, __VA_ARGS__);                               \
-            }                                                                                                                  \
-        }                                                                                                                      \
+#define APPLY_HORIZONTAL_SHIFT_8X1(fn, ...)                                                            \
+    do {                                                                                               \
+        if (out_of_boundary_left >= 0 || out_of_boundary_right >= 0) {                                 \
+            for (int k = 0; k < 15; ++k) {                                                             \
+                const int iy = clamp(iy4 + k - 7, 0, height - 1);                                      \
+                                                                                                       \
+                const uint8_t *idx2b = ref2b + iy * stride2b + ix4 - 7;                                \
+                const uint8_t *idx8b = ref8b + iy * stride8b + ix4 - 7;                                \
+                uint8x16_t     src2b = vld1q_u8(idx2b);                                                \
+                uint8x16_t     src8b = vld1q_u8(idx8b);                                                \
+                uint16x8x2_t   src_1;                                                                  \
+                src_1.val[0] = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(src2b, src8b)), 6);          \
+                src_1.val[1] = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(src2b, src8b)), 6);          \
+                                                                                                       \
+                src_1         = clamp_horizontal(src_1,                                                \
+                                         out_of_boundary_left,                                 \
+                                         out_of_boundary_right,                                \
+                                         ref2b,                                                \
+                                         ref8b,                                                \
+                                         iy,                                                   \
+                                         stride2b,                                             \
+                                         stride8b,                                             \
+                                         width,                                                \
+                                         indx0,                                                \
+                                         indx1);                                               \
+                int16x8_t rv0 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 0);      \
+                int16x8_t rv1 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 1);      \
+                int16x8_t rv2 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 2);      \
+                int16x8_t rv3 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 3);      \
+                int16x8_t rv4 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 4);      \
+                int16x8_t rv5 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 5);      \
+                int16x8_t rv6 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 6);      \
+                int16x8_t rv7 = vextq_s16(                                                             \
+                    vreinterpretq_s16_u16(src_1.val[0]), vreinterpretq_s16_u16(src_1.val[1]), 7);      \
+                tmp[k] = (fn)(rv0, rv1, rv2, rv3, rv4, rv5, rv6, rv7, __VA_ARGS__);                    \
+            }                                                                                          \
+        } else {                                                                                       \
+            for (int k = 0; k < 15; ++k) {                                                             \
+                const int iy = clamp(iy4 + k - 7, 0, height - 1);                                      \
+                                                                                                       \
+                const uint8_t *idx2b  = ref2b + iy * stride2b + ix4 - 7;                               \
+                const uint8_t *idx8b  = ref8b + iy * stride8b + ix4 - 7;                               \
+                uint8x16_t     src2b  = vld1q_u8(idx2b);                                               \
+                uint8x16_t     src8b  = vld1q_u8(idx8b);                                               \
+                uint16x8_t     src_lo = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(src2b, src8b)), 6); \
+                uint16x8_t     src_hi = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(src2b, src8b)), 6); \
+                                                                                                       \
+                int16x8_t rv0 = vreinterpretq_s16_u16(src_lo);                                         \
+                int16x8_t rv1 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 1));                   \
+                int16x8_t rv2 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 2));                   \
+                int16x8_t rv3 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 3));                   \
+                int16x8_t rv4 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 4));                   \
+                int16x8_t rv5 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 5));                   \
+                int16x8_t rv6 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 6));                   \
+                int16x8_t rv7 = vreinterpretq_s16_u16(vextq_u16(src_lo, src_hi, 7));                   \
+                tmp[k]        = (fn)(rv0, rv1, rv2, rv3, rv4, rv5, rv6, rv7, __VA_ARGS__);             \
+            }                                                                                          \
+        }                                                                                              \
     } while (0)
 
     if (p_width == 4) {
@@ -420,10 +468,11 @@ static AOM_FORCE_INLINE void warp_affine_vertical(uint16_t *pred, int p_width, i
     }
 }
 
-static AOM_FORCE_INLINE void highbd_warp_affine_common(const int32_t *mat, const uint16_t *ref, int width, int height,
-                                                       int stride, uint16_t *pred, int p_col, int p_row, int p_width,
-                                                       int p_height, int p_stride, int subsampling_x, int subsampling_y,
-                                                       int bd, ConvolveParams *conv_params, int16_t alpha, int16_t beta,
+static AOM_FORCE_INLINE void highbd_warp_affine_common(const int32_t *mat, const uint8_t *ref8b, const uint8_t *ref2b,
+                                                       int width, int height, int stride8b, int stride2b,
+                                                       uint16_t *pred, int p_col, int p_row, int p_width, int p_height,
+                                                       int p_stride, int subsampling_x, int subsampling_y, int bd,
+                                                       ConvolveParams *conv_params, int16_t alpha, int16_t beta,
                                                        int16_t gamma, int16_t delta) {
     uint16_t *const dst                   = conv_params->dst;
     const int       dst_stride            = conv_params->dst_stride;
@@ -473,7 +522,8 @@ static AOM_FORCE_INLINE void highbd_warp_affine_common(const int32_t *mat, const
             // So it is safe to use int16x8_t as the intermediate storage type here.
             int16x8_t tmp[15];
 
-            warp_affine_horizontal(ref, width, height, stride, p_width, alpha, beta, iy4, sx4, ix4, tmp, bd);
+            warp_affine_horizontal(
+                ref8b, ref2b, width, height, stride8b, stride2b, p_width, alpha, beta, iy4, sx4, ix4, tmp, bd);
             warp_affine_vertical(pred,
                                  p_width,
                                  p_height,
diff --git a/Source/Lib/ASM_SVE/highbd_warp_plane_sve.c b/Source/Lib/ASM_SVE/highbd_warp_plane_sve.c
index f45f5231..7335cdff 100644
--- a/Source/Lib/ASM_SVE/highbd_warp_plane_sve.c
+++ b/Source/Lib/ASM_SVE/highbd_warp_plane_sve.c
@@ -229,13 +229,13 @@ void svt_av1_highbd_warp_affine_sve(const int32_t *mat, const uint8_t *ref8b, co
                                     int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y,
                                     int bd, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma,
                                     int16_t delta) {
-    uint16_t *ref = svt_aom_memalign(32, sizeof(uint16_t) * width * height);
-    svt_enc_msb_pack2d_neon((uint8_t *)ref8b, stride8b, (uint8_t *)ref2b, ref, stride2b, width, width, height);
     highbd_warp_affine_common(mat,
-                              ref,
+                              ref8b,
+                              ref2b,
                               width,
                               height,
-                              width,
+                              stride8b,
+                              stride2b,
                               pred,
                               p_col,
                               p_row,
@@ -250,5 +250,4 @@ void svt_av1_highbd_warp_affine_sve(const int32_t *mat, const uint8_t *ref8b, co
                               beta,
                               gamma,
                               delta);
-    svt_aom_free(ref);
 }
-- 
2.36.0.windows.1

